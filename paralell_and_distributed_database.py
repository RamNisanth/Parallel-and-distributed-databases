# -*- coding: utf-8 -*-
"""Paralell_and_distributed_database.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16n6B1aBEJJlq-lfncy8X3mQm-dRaYU9d
"""

!pip install boto3

import boto3
from getpass import getpass


aws_access_key_id = getpass('') #access key
aws_secret_access_key = getpass('') #secret key
region_name = 'us-east-2'  # example: 'us-east-1'

# Initialize a session using Boto3
session = boto3.Session(
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name=region_name
)

# Create an S3 client
s3 = session.client('s3')

from io import BytesIO

import pandas as pd
import io

bucket_name = 'vaishali-e-commerce-data-analysis'  # replace with your bucket name
object_key = 'kaggle_ecommerce_complete.csv'  # replace with your file path
response = s3.get_object(Bucket=bucket_name, Key=object_key)
raw_data = response['Body'].read()
encoding = 'ISO-8859-1'
# Read the file using the detected encoding
#print(raw_data)
# data = pd.read_csv((raw_data))
# Assuming 'raw_data' contains the byte data of the CSV file
# Convert the raw byte data into a file-like object
file_like_object = io.BytesIO(raw_data)

# Read the CSV file using pd.read_csv()
# data = pd.read_csv(file_like_object)
data = pd.read_csv(file_like_object, encoding='ISO-8859-1')


# Now you can work with the DataFrame 'data'
print(data.head())

!pip install pyspark py4j

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Pandas to PySpark Integration") \
    .getOrCreate()

spark_df = spark.createDataFrame(data)

spark_df.show()

spark_df.createOrReplaceTempView("ecommerce_data")
result = spark.sql("SELECT * FROM ecommerce_data WHERE rank > 10")
result.show()

# Data preprocessing
from pyspark.sql.functions import isnull

# Apply isnull() method to identify null values
null_df = spark_df.select([isnull(column).alias(column) for column in spark_df.columns])

# Show the DataFrame with boolean values indicating null or not null
null_df.show()

from pyspark.sql.functions import col, count, when

# Calculate the count of non-null values for each column
non_null_counts = [count(when(col(c).isNotNull(), c)).alias(c) for c in spark_df.columns]

# Create a DataFrame with the non-null counts
non_null_counts_df = spark_df.agg(*non_null_counts)

# Calculate the data types of each column
data_types = [col(c).cast("string").alias(c) for c in spark_df.columns]

# Create a DataFrame with the data types
data_types_df = spark_df.select(*data_types)

# Combine the non-null counts and data types DataFrames
summary_df = non_null_counts_df.unionByName(data_types_df)

# Show the summary DataFrame
summary_df.show()

from pyspark.sql.functions import col
# Drop columns
columns_to_drop = ['product_image', 'product_link', 'url']  # List of column names to drop
spark_df = spark_df.drop(*columns_to_drop)

spark_df.show()

# dropping rows if they contain any null or blank values
spark_df = spark_df.na.drop(how='any', subset=["_unit_id", "relevance", "product_price", "rank"])

# rounding the number of decimal points to 2 for easy calsulations
from pyspark.sql.functions import round

columns_to_round = ['relevance', 'relevance:variance']
decimal_places = 2

# Round the specified columns to 2 decimal places
rounded_df = spark_df.select(*[round(col_name, decimal_places).alias(col_name) for col_name in columns_to_round])

rounded_df.show()

# Plot bar charts for query, rank, and source
import matplotlib.pyplot as plt

categorical_columns = ['query', 'rank', 'source']

for column in categorical_columns:
    counts = spark_df.groupBy(column).count().orderBy('count', ascending=False).toPandas()
    plt.bar(counts[column], counts['count'])
    plt.title(f'Bar Chart of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)
    plt.grid(True)
    plt.show()

# Remove rows with null values in the product_price column
spark_df = spark_df.na.drop(subset=['product_price'])

from pyspark.sql.functions import col, length
from pyspark.sql.types import FloatType
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator

# Convert product_price column to FloatType
selected_columns = ['relevance', 'relevance:variance', 'rank']

# Select the specified columns
selected_df = spark_df.select(selected_columns)

# Prepare the data for regression
# Since Linear Regression requires numerical features, we need to assemble the features into a single vector
assembler = VectorAssembler(inputCols=selected_columns, outputCol="features")

# Transform the data
transformed_data = assembler.transform(selected_df)

# Split the data into training and testing sets
train_data, test_data = transformed_data.randomSplit([0.8, 0.2], seed=42)

# Define the Linear Regression model
regression_model = LinearRegression(featuresCol="features", labelCol="relevance")

# Train the model
model = regression_model.fit(train_data)

# Make predictions on the test data
predictions = model.transform(test_data)

# Evaluate the model
evaluator = RegressionEvaluator(labelCol="relevance", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data:", rmse)